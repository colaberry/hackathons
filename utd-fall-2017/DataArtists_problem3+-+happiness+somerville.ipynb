{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "s1",
     "content"
    ]
   },
   "source": [
    "\n",
    "                \n",
    "# Problem 3: Somerville Happiness Index  \n",
    "                \n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading dataset in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_hap = pd.read_csv('https://data.somervillema.gov/api/views/yevj-2b33/rows.csv',delimiter=',', header=0,na_values=[\" \"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking the number of rows and columns in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_hap.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## counting the null values in each of the dataframe columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_hap.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop columns where missing data is more than 50 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_hap_rmcol=df_hap[df_hap.columns[df_hap.isnull().mean() < 0.5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## as we see below we are left only with 17 columns after the removal of missing data columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_hap_rmcol.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking the sum of null values in the updated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_hap_rmcol.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## column transformation : cleaning and making column names more understandable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_hap_rmcol.rename(columns={'How.happy.do.you.feel.right.now.': 'happiness_score', \n",
    "                             'How.satisfied.are.you.with.your.life.in.general.': 'satisfaction_general',\n",
    "                             'How.satisfied.are.you.with.Somerville.as.a.place.to.live.': 'satisfaction_somerville',\n",
    "                             'In.general..how.similar.are.you.to.other.people.you.know._2011': 'similarity_2011',\n",
    "                             'When.making.decisions..are.you.more.likely.to.seek.advice.or.decide.for.yourself._2011.':'decision_making',\n",
    "                             'The.availability.of.affordable.housing_2011':'housing_availability_2011',\n",
    "                             'How.would.you.rate.the.following..The.overall.quality.of.public.schools.in.your.community._2011':'school_quality_2011',\n",
    "                             'How.would.you.rate.the.following..The.beauty.or.physical.setting_2011':'beauty_physical_setting',\n",
    "                             'How.would.you.rate.the.following..The.effectiveness.of.the.local.police_2011_2013':'police_effectiveness',\n",
    "                             'What.is.your.gender._2011':'gender_2011',\n",
    "                             'Age.':'age',\n",
    "                             'Marital.status._2011':'marital_status_2011',\n",
    "                             'What.is.your.race_2011_2013':'race_2011_2013',\n",
    "                             'How.long.have.you.lived.here.':'living_time',\n",
    "                             'What.is.your.annual.household.income.':'income'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking the updated column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_hap_rmcol.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imputation : here we impute the null column values with the most frequent value of that respective column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_imputed = df_hap_rmcol.apply(lambda x:x.fillna(x.value_counts().index[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## after the cleaning we have no null values in the data ! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_imputed.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking the first 10 rows of the clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_imputed.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating the dummy variables for achieving the one hot encoding of categorical columns using get_dummies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df_imputed, columns=['gender_2011',\n",
    " 'age',\n",
    " 'marital_status_2011',\n",
    " 'race_2011_2013',\n",
    " 'living_time',\n",
    " 'income'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking columns after the one hot encoding of categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dummies.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## removing the non-numeric noisy data from the happiness_score column as it is our dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_numeric = df_dummies[df_dummies.happiness_score.apply(lambda x: x.isnumeric())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making sure all non-numeric data is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_numeric['happiness_score'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_numeric.to_csv('num.csv', sep=',', encoding='utf-8')\n",
    "df_num = pd.read_csv('num.csv',delimiter=',', header=0,na_values=[\" \"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to creat the happiness score value as 0: unhappy and 1: happy\n",
    "## people with happiness score < 6 are unhappy and people with score > 6 are happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def category(row):\n",
    "    if row['happiness_score'] < 6:\n",
    "        val = '0'\n",
    "    else:\n",
    "        val = '1'\n",
    "    return val\n",
    "\n",
    "\n",
    "df_num['hapiness_bin'] = df_num.apply(category, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dropping the actual happiness score column since we created a new binary column with values 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_num1 = df_num.drop(df_num['happiness_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## since some of the columns are still having some noisy data we get rid of those and retain only the numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_columns = df_num1.columns.tolist()\n",
    "df_num2 = (df_num1.drop(data_columns, axis=1)\n",
    "         .join(df_num1[data_columns].apply(pd.to_numeric, errors='coerce')))\n",
    "\n",
    "df_num2 = df_num2[df_num2[data_columns].notnull().all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finally we check the distribution of happy vs non-happy people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_num2['hapiness_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating the X and y variables\n",
    "## X contains all variables except the dependent variable\n",
    "## y contains our dependent variable 'happiness_bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X= df_num2.drop(['hapiness_bin'], axis=1)\n",
    "y = df_num2['hapiness_bin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## after this we split the data into training and testing data with a 75 and 25 train test split\n",
    "## we create a logistic regression classifier\n",
    "## then we fit the logistic regressor on the training data with class_weight = 'balanced' since we saw that we have imbalanaced dataset\n",
    "## after that we predict on the test data\n",
    "## finally we compute the auc score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=1)\n",
    "mul_lr_balanced = linear_model.LogisticRegression(class_weight='balanced')\n",
    "mul_lr_balanced.fit(X_train, y_train)\n",
    "#print (\"accuracy:\",mul_lr_balanced.score(X_test, y_test))   \n",
    "y_pred = mul_lr_balanced.predict(X_test)\n",
    "roc_auc_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking number of 1's and 0's to make sure model is not predicting only 1 for all records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " df_pred['pred']= pd.DataFrame(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_pred['pred'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similarly creating a decision classifier \n",
    "## fitting it on training data\n",
    "## checking the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=1)\n",
    "        \n",
    "decision_tree_classifier = DecisionTreeClassifier(class_weight='balanced')\n",
    "        \n",
    "decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "print (\"accuracy:\",decision_tree_classifier.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we want to implement a multi-class logit \n",
    "## so we create 3 categories of happiness : 1: unhappy, 2: somewhat happy 3: very happy\n",
    "## for that we create a fucntion and make a new column with these 3 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 categories\n",
    "def category3(row):\n",
    "    if row['happiness_score'] < 4:\n",
    "        val = '1'\n",
    "    elif row['happiness_score'] < 7:\n",
    "        val = '2'\n",
    "    else:\n",
    "        val = '3'\n",
    "    return val\n",
    "\n",
    "\n",
    "df_num2['hapiness_bin3'] = df_num.apply(category3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X= df_num2.drop(['hapiness_bin3'], axis=1)\n",
    "y = df_num2['hapiness_bin3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## again we check the distribution of all the three categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now we do the same but here we do multiclass logit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=1)\n",
    "mul_lr_balanced = linear_model.LogisticRegression(multi_class='multinomial',class_weight='balanced',solver='newton-cg')\n",
    "mul_lr_balanced.fit(X_train, y_train)\n",
    "a = mul_lr_balanced.score(X_test, y_test)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is assertion block which verifies the solution.\n",
    "\n",
    "try:\n",
    "    def verify_answer():\n",
    "\n",
    "        if a > 0.9:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    ref_assert_var = verify_answer()\n",
    "except Exception as e:\n",
    "    print('Your assertion block throws error: ' + str(e))\n",
    "else:\n",
    "    if ref_assert_var:\n",
    "        print('looks good :)')\n",
    "    else:\n",
    "        print('The answer did not pass the test. Try again')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we then run a decision tree as well on the multiclass data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=1)\n",
    "        \n",
    "decision_tree_classifier = DecisionTreeClassifier(class_weight='balanced')\n",
    "        \n",
    "decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "print (\"accuracy:\",decision_tree_classifier.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "executed_sections": [],
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
